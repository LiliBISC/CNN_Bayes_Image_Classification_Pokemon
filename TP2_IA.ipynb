{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2_IA",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Bayes Point machine et classification dâ€™images"
      ],
      "metadata": {
        "id": "FikOcObPO16l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFstC4ctRNlV",
        "outputId": "0bd32df9-72fe-45ed-c9ea-9546cd201f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mCyiURH4OsRj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "848fe0f3-3c55-4777-e03a-29d1fc7b1a40"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ddc33db14124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mimportData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"archive.zip\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"https://github.com/LiliBISC/Bayes_Point_machine-classification_images/blob/main/archive.zip?raw=true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-ddc33db14124>\u001b[0m in \u001b[0;36mimportData\u001b[0;34m(name, url)\u001b[0m\n\u001b[1;32m     12\u001b[0m     extract=False)\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import splitfolders \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#Import database\n",
        "\n",
        "def importData(name, url):\n",
        "  data = tf.keras.utils.get_file(\n",
        "    name,\n",
        "    url,\n",
        "    extract=False)\n",
        "  import zipfile\n",
        "  with zipfile.ZipFile(data, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n",
        "  return 0\n",
        "\n",
        "\n",
        "importData(\"archive.zip\",\"https://github.com/LiliBISC/Bayes_Point_machine-classification_images/blob/main/archive.zip?raw=true\")\n",
        "\n",
        "\n",
        "input_dir = os.path.join('/content/archive/flowers/')\n",
        "output_dir = os.path.join('/content/archive/flowers_splitted/')\n",
        "\n",
        "splitfolders.ratio(input_dir, output=output_dir, seed=1337, ratio=(.8, .2), group_prefix=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join('path/to/your/folder/flowers_splitted/train/')\n",
        "test_dir = os.path.join('path/to/your/folder/flowers_splitted/val/')"
      ],
      "metadata": {
        "id": "bsN5gFi3R2Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def image_generator(train_parent_directory, test_parent_directory):\n",
        "    \n",
        "    train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1/255)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(train_parent_directory,\n",
        "                                  target_size = (75,75),\n",
        "                                  batch_size = 214,\n",
        "                                  class_mode = 'categorical',\n",
        "                                  subset='training')\n",
        " \n",
        "    \n",
        "    test_generator = test_datagen.flow_from_directory(test_parent_directory,\n",
        "                                 target_size=(75,75),\n",
        "                                 batch_size = 37,\n",
        "                                 class_mode = 'categorical')    \n",
        "    \n",
        "    return train_generator, test_generator"
      ],
      "metadata": {
        "id": "yU1kP4qsR3JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, test_generator = image_generator(train_dir, test_dir)"
      ],
      "metadata": {
        "id": "6IDnHzzrR_hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(input_shape=(75,75,3), filters=8, kernel_size=16, activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(units=5, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lFvzvBTRSCtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "0VHkLcHBSGXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=10,  \n",
        "      epochs=100,\n",
        "      verbose=1)"
      ],
      "metadata": {
        "id": "5fq9WVIcV8TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def import_and_predict(image_data, label):\n",
        "    \n",
        "    #read image\n",
        "    img = cv2.imread(image_data)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
        "    \n",
        "    #show the image\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # resize and reshape the image\n",
        "    img_resize = (cv2.resize(img, dsize=(75, 75), interpolation=cv2.INTER_CUBIC))/255.\n",
        "    \n",
        "    img_reshape = img_resize[np.newaxis,...]\n",
        "    \n",
        "    #predict the image\n",
        "    prediction = model.predict(img_reshape)\n",
        "    print(prediction)\n",
        "    \n",
        "    label_prediction = label[np.argmax(prediction)]\n",
        "    \n",
        "    return label_prediction\n",
        " \n",
        "label = os.listdir(test_dir)\n",
        "image1_dir = os.path.join(test_dir+'dandelion/3696596109_4c4419128a_m.jpg')\n",
        "\n",
        "prediction = import_and_predict(image1_dir, label)"
      ],
      "metadata": {
        "id": "p18GXIAAV87U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-probability "
      ],
      "metadata": {
        "id": "ESnTI1wHV9N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers"
      ],
      "metadata": {
        "id": "ydueyMDBWIHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "divergence_fn = lambda q,p,_:tfd.kl_divergence(q,p)/3457\n",
        "\n",
        "model_bayes = Sequential([\n",
        "    \n",
        "    tfpl.Convolution2DReparameterization(input_shape=(75,75,3), filters=8, kernel_size=16, activation='relu',\n",
        "                                           kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
        "                                           kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
        "                                           kernel_divergence_fn = divergence_fn,\n",
        "                                           bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
        "                                           bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
        "                                           bias_divergence_fn = divergence_fn),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    tfpl.DenseReparameterization(units=tfpl.OneHotCategorical.params_size(5), activation=None,\n",
        "                                    kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
        "                                    kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
        "                                    kernel_divergence_fn = divergence_fn,\n",
        "                                    bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
        "                                    bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
        "                                    bias_divergence_fn = divergence_fn\n",
        "                                ),\n",
        "    tfpl.OneHotCategorical(5)\n",
        "    \n",
        "])\n",
        "model_bayes.summary()"
      ],
      "metadata": {
        "id": "Y9zXXdTvWijd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def negative_log_likelihood(y_true, y_pred):\n",
        "    return -y_pred.log_prob(y_true)\n",
        "\n",
        "model_bayes.compile(loss = negative_log_likelihood,\n",
        "              optimizer = Adam(learning_rate=0.005),\n",
        "              metrics = ['accuracy'],\n",
        "              experimental_run_tf_function = False)"
      ],
      "metadata": {
        "id": "G0kTCClPWxzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_bayes = model_bayes.fit(\n",
        "      train_generator,\n",
        "      epochs=300,\n",
        "      verbose=1)"
      ],
      "metadata": {
        "id": "1NAN1sLGWycF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def import_and_predict_bayes(image, true_label):\n",
        "\n",
        "    #read image\n",
        "    img = cv2.imread(image)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
        "    \n",
        "    #show the image\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    img_resize = (cv2.resize(img, dsize=(75, 75), interpolation=cv2.INTER_CUBIC))/255.\n",
        "    \n",
        "    predicted_probabilities = np.empty(shape=(300, 5))\n",
        "    \n",
        "    for i in range(300):\n",
        "        \n",
        "        predicted_probabilities[i] = model_bayes(img_resize[np.newaxis,...]).mean().numpy()[0]\n",
        "        \n",
        "    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(5)])\n",
        "    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 97.5) for i in range(5)])\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    bar = ax.bar(np.arange(5), pct_97p5, color='red')\n",
        "    bar[true_label].set_color('green')\n",
        "    bar = ax.bar(np.arange(5), pct_2p5-0.02, color='white')\n",
        "    ax.set_xticklabels([''] + [x for x in label])\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_ylabel('Probability')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cmapIXZKW0rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tiEqcuukW397"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}